{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-29T20:49:19.589091Z","iopub.execute_input":"2022-08-29T20:49:19.589966Z","iopub.status.idle":"2022-08-29T20:49:23.568566Z","shell.execute_reply.started":"2022-08-29T20:49:19.589389Z","shell.execute_reply":"2022-08-29T20:49:23.567658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the necessary dependencies\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nfrom PIL import Image\nfrom kaggle_datasets import KaggleDatasets","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:23.570300Z","iopub.execute_input":"2022-08-29T20:49:23.570646Z","iopub.status.idle":"2022-08-29T20:49:29.629620Z","shell.execute_reply.started":"2022-08-29T20:49:23.570619Z","shell.execute_reply":"2022-08-29T20:49:29.628550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Connecting to TPU","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:29.631070Z","iopub.execute_input":"2022-08-29T20:49:29.631828Z","iopub.status.idle":"2022-08-29T20:49:35.290305Z","shell.execute_reply.started":"2022-08-29T20:49:29.631795Z","shell.execute_reply":"2022-08-29T20:49:35.289656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path() #Google cloud storage path\nMONET_Filenames = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nTEST_Photo_Filenames = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nIMAGE_SIZE = [256, 256] #original size of image\nBATCH_SIZE = 1\nOUTPUT_CHANNELS = 3 #RGB \nEPOCHS = 20 #No of iterations for the model\n#Note:Higher Epochs give you a better result as the loss value keeps decreasing.\n# I have gone for 20 as I have very low computational power","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:35.291753Z","iopub.execute_input":"2022-08-29T20:49:35.292410Z","iopub.status.idle":"2022-08-29T20:49:35.836325Z","shell.execute_reply.started":"2022-08-29T20:49:35.292379Z","shell.execute_reply":"2022-08-29T20:49:35.835401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Loading Datasets","metadata":{}},{"cell_type":"markdown","source":"**Displaying pictures**","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(3,3, figsize=(8,8))\nplt.suptitle('Some images with monet style', fontsize=19, fontweight='bold')\n\nind = 0 \nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(Image.open('../input/gan-getting-started/monet_jpg/'+os.listdir('../input/gan-getting-started/monet_jpg')[ind]))\n        ind += 1","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:35.837520Z","iopub.execute_input":"2022-08-29T20:49:35.837793Z","iopub.status.idle":"2022-08-29T20:49:37.406867Z","shell.execute_reply.started":"2022-08-29T20:49:35.837755Z","shell.execute_reply":"2022-08-29T20:49:37.406208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(3,3, figsize=(8,8))\nplt.suptitle('Some images with no monet style', fontsize=19, fontweight='bold')\n\nind = 0 \nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(Image.open('../input/gan-getting-started/photo_jpg/'+os.listdir('../input/gan-getting-started/photo_jpg')[ind]))\n        ind += 1","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:37.408106Z","iopub.execute_input":"2022-08-29T20:49:37.408522Z","iopub.status.idle":"2022-08-29T20:49:38.695564Z","shell.execute_reply.started":"2022-08-29T20:49:37.408475Z","shell.execute_reply":"2022-08-29T20:49:38.694870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MONET_filenames = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nprint(f'{len(MONET_filenames)} Monet TFRecord Files')\nTest_Photo_filenames=tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint(f'{len(Test_Photo_filenames)} Test_Photo TFRecord Files')","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:38.696779Z","iopub.execute_input":"2022-08-29T20:49:38.697189Z","iopub.status.idle":"2022-08-29T20:49:38.858713Z","shell.execute_reply.started":"2022-08-29T20:49:38.697139Z","shell.execute_reply":"2022-08-29T20:49:38.857841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data augmentation**: it a technique to increase the diversity of the training set by applying random (but realistic) transformations, such as image rotation, and it can be done very easily using the API tf.image. <br> To learn more about it check out the official decantation:\nhttps://www.tensorflow.org/tutorials/images/data_augmentation.","metadata":{}},{"cell_type":"code","source":"def data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32) #generates random uniform distribution\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n\n    \n    #rotating images\n    if p_rotate > .8:\n        image = tf.image.rot90(image, k=3)  \n    elif p_rotate > .6:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .4:\n        image = tf.image.rot90(image, k=1)\n        \n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    #transposing images\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n    image = tf.image.random_hue(image, 0.01) #hue settings\n    image = tf.image.random_saturation(image, 0.70, 1.30) #saturation settings\n    image = tf.image.random_contrast(image, 0.80, 1.20) #contrast settings\n    image = tf.image.random_brightness(image, 0.10) #brightness settings\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:38.860396Z","iopub.execute_input":"2022-08-29T20:49:38.860897Z","iopub.status.idle":"2022-08-29T20:49:38.871154Z","shell.execute_reply.started":"2022-08-29T20:49:38.860856Z","shell.execute_reply":"2022-08-29T20:49:38.870516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Displaying images after applying data augmentation**","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(3,3, figsize=(8,8))\nplt.suptitle('Some augmented images with monet style', fontsize=19, fontweight='bold')\n\nind = 0 \nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(data_augment(np.array(Image.open('../input/gan-getting-started/monet_jpg/'+os.listdir('../input/gan-getting-started/monet_jpg')[ind]))))\n        ind += 1","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:38.872402Z","iopub.execute_input":"2022-08-29T20:49:38.873231Z","iopub.status.idle":"2022-08-29T20:49:40.310600Z","shell.execute_reply.started":"2022-08-29T20:49:38.873196Z","shell.execute_reply":"2022-08-29T20:49:40.309669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(3,3, figsize=(8,8))\nplt.suptitle('Some images with no monet style', fontsize=19, fontweight='bold')\n\nind = 0 \nfor i in range(3):\n    for j in range(3):\n        ax[i][j].imshow(Image.open('../input/gan-getting-started/photo_jpg/'+os.listdir('../input/gan-getting-started/photo_jpg')[ind]))\n        ind += 1","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:40.313204Z","iopub.execute_input":"2022-08-29T20:49:40.313475Z","iopub.status.idle":"2022-08-29T20:49:41.606596Z","shell.execute_reply.started":"2022-08-29T20:49:40.313445Z","shell.execute_reply":"2022-08-29T20:49:41.605765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's define a function decode_image that decode a JPEG-encoded image to a uint8 tensor, casts it to a float32, divide it by 127.5 and subtract it by 1, to make the values in the tensor between -1 and 1, and finally, reshape it to (IMAGE_SIZE,IMAGE_SIZE, NUM_CHANNEL)","metadata":{}},{"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:41.607981Z","iopub.execute_input":"2022-08-29T20:49:41.608389Z","iopub.status.idle":"2022-08-29T20:49:41.614890Z","shell.execute_reply.started":"2022-08-29T20:49:41.608351Z","shell.execute_reply":"2022-08-29T20:49:41.613971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's define a function read_tfrecord to Parse a single Example photo.","metadata":{}},{"cell_type":"code","source":"def read_tfrecord(example):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:41.616493Z","iopub.execute_input":"2022-08-29T20:49:41.616816Z","iopub.status.idle":"2022-08-29T20:49:41.626575Z","shell.execute_reply.started":"2022-08-29T20:49:41.616780Z","shell.execute_reply":"2022-08-29T20:49:41.625638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's define a function load_dataset to load our dataset.","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:41.628091Z","iopub.execute_input":"2022-08-29T20:49:41.628595Z","iopub.status.idle":"2022-08-29T20:49:41.640761Z","shell.execute_reply.started":"2022-08-29T20:49:41.628553Z","shell.execute_reply":"2022-08-29T20:49:41.639696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ddefining a function **get_gan_dataset** that load datasets from monet_files and photo_files, then augments the data with the function that we defined earlier **data_augment** and combines consecutive elements of this dataset into batches, then let's use prefetch to allow later elements to be prepared while the current element is being processed. This improves the latency and throughput, at the cost of using additional memory to store prefetched elements. Finally , let's create our final Dataset by zipping together the given datasets (monet_ds and photo_ds).","metadata":{}},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_Filenames).batch(1)\nphoto_ds = load_dataset(TEST_Photo_Filenames).batch(1)\ndef get_gan_dataset(monet_files, photo_files, batch_size=BATCH_SIZE):\n\n    monet_ds = load_dataset(monet_files)\n    photo_ds = load_dataset(photo_files)\n    \n    monet_ds = monet_ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n    photo_ds = photo_ds.map(data_augment, num_parallel_calls=AUTOTUNE)\n        \n    monet_ds = monet_ds.batch(batch_size)\n    photo_ds = photo_ds.batch(batch_size)\n    \n    monet_ds = monet_ds.prefetch(AUTOTUNE)\n    photo_ds = photo_ds.prefetch(AUTOTUNE)\n    \n    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n    \n    return gan_ds\n\nfinal_dataset = get_gan_dataset(MONET_Filenames, TEST_Photo_Filenames, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:41.642264Z","iopub.execute_input":"2022-08-29T20:49:41.642615Z","iopub.status.idle":"2022-08-29T20:49:42.455215Z","shell.execute_reply.started":"2022-08-29T20:49:41.642577Z","shell.execute_reply":"2022-08-29T20:49:42.454401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Models and Losses","metadata":{}},{"cell_type":"markdown","source":"**Constructing the Generator**","metadata":{}},{"cell_type":"code","source":"#Defining the downsample layer \ndef down_sample(filters, size, apply_instancenorm=True):\n    # In the paper the weights are initialized from a Gaussian distribution N (0, 0.02).\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    layer = keras.Sequential()\n    layer.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        layer.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    layer.add(layers.LeakyReLU())\n\n    return layer","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:42.456587Z","iopub.execute_input":"2022-08-29T20:49:42.456908Z","iopub.status.idle":"2022-08-29T20:49:42.463064Z","shell.execute_reply.started":"2022-08-29T20:49:42.456869Z","shell.execute_reply":"2022-08-29T20:49:42.462251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining the upsampling layer\ndef up_sample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    layer = keras.Sequential()\n    layer.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer,use_bias=False))\n    layer.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        layer.add(layers.Dropout(0.5))\n\n    layer.add(layers.ReLU())\n\n    return layer","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:42.464185Z","iopub.execute_input":"2022-08-29T20:49:42.464532Z","iopub.status.idle":"2022-08-29T20:49:42.475602Z","shell.execute_reply.started":"2022-08-29T20:49:42.464505Z","shell.execute_reply":"2022-08-29T20:49:42.474748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compiling the generator\ndef Generator():\n    inputs = layers.Input(shape=[256,256,3])\n    down_stack = [\n        down_sample(64, 4, apply_instancenorm=False),\n        down_sample(128, 4),                        \n        down_sample(256, 4),                        \n        down_sample(512, 4),                        \n        down_sample(512, 4),                      \n        down_sample(512, 4),                      \n        down_sample(512, 4),                      \n        down_sample(512, 4),                      \n    ]\n\n    up_stack = [\n        up_sample(512, 4, apply_dropout=True),    \n        up_sample(512, 4, apply_dropout=True),    \n        up_sample(512, 4, apply_dropout=True),    \n        up_sample(512, 4),                          \n        up_sample(256, 4),                         \n        up_sample(128, 4),                           \n        up_sample(64, 4),                           \n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    # The last activaltion function is tanh because we want to force the model to generate pixels between 1 and -1, to be the same as the input pixels after preprocessing.\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh') \n   \n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:42.477075Z","iopub.execute_input":"2022-08-29T20:49:42.477486Z","iopub.status.idle":"2022-08-29T20:49:42.490521Z","shell.execute_reply.started":"2022-08-29T20:49:42.477453Z","shell.execute_reply":"2022-08-29T20:49:42.489779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Constructing the Discriminator**","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    \n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n    x = inp\n    \n    down1 = down_sample(64, 4, False)(x)       \n    down2 = down_sample(128, 4)(down1)        \n    down3 = down_sample(256, 4)(down2)        \n\n    zero_pad1 = layers.ZeroPadding2D()(down3)\n    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n    leaky_relu = layers.LeakyReLU()(norm1)\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:42.491543Z","iopub.execute_input":"2022-08-29T20:49:42.491924Z","iopub.status.idle":"2022-08-29T20:49:42.504595Z","shell.execute_reply.started":"2022-08-29T20:49:42.491882Z","shell.execute_reply":"2022-08-29T20:49:42.503936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's create the models in strategy.scope() for TPU","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = Generator() # transforms photos to Monet style \n    photo_generator = Generator() # transforms Monet style to be more like photos\n\n    monet_discriminator = Discriminator() # differentiates real images with Monet style andi mages with generated Monet style\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:42.505569Z","iopub.execute_input":"2022-08-29T20:49:42.506313Z","iopub.status.idle":"2022-08-29T20:49:52.222409Z","shell.execute_reply.started":"2022-08-29T20:49:42.506283Z","shell.execute_reply":"2022-08-29T20:49:52.221650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building CycleGAN class**","metadata":{}},{"cell_type":"code","source":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator, \n        lambda_cycle=10, \n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn,\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        batch_size = tf.shape(real_monet)[0]\n        with tf.GradientTape(persistent=True) as tape:\n        \n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            \n            \n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.223528Z","iopub.execute_input":"2022-08-29T20:49:52.223728Z","iopub.status.idle":"2022-08-29T20:49:52.240807Z","shell.execute_reply.started":"2022-08-29T20:49:52.223704Z","shell.execute_reply":"2022-08-29T20:49:52.239946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Defining the loss function for the discriminator that labels the original images as 1, and the false ones as 0. The ideal discriminator will output only  1 for the true images and  zeros for the false.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.241889Z","iopub.execute_input":"2022-08-29T20:49:52.242287Z","iopub.status.idle":"2022-08-29T20:49:52.254528Z","shell.execute_reply.started":"2022-08-29T20:49:52.242258Z","shell.execute_reply":"2022-08-29T20:49:52.253422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Defining the loss function for the generator that tries to trick the discriminator by generating an image that the discriminator considers as original. An ideal generator will cause the discriminator on the output to return  1.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.255793Z","iopub.execute_input":"2022-08-29T20:49:52.256114Z","iopub.status.idle":"2022-08-29T20:49:52.268230Z","shell.execute_reply.started":"2022-08-29T20:49:52.256087Z","shell.execute_reply":"2022-08-29T20:49:52.267326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Defining the Cycle consistency loss, wich is the arithmetic mean of the differences between the original photo and the transformed photo denoted as l1","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        l1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * l1","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.269319Z","iopub.execute_input":"2022-08-29T20:49:52.269655Z","iopub.status.idle":"2022-08-29T20:49:52.277751Z","shell.execute_reply.started":"2022-08-29T20:49:52.269627Z","shell.execute_reply":"2022-08-29T20:49:52.277045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally defining thr Identity loss that is used to compare the image x and that image produced by generator F. We expect F (x) ~ x, i.e. if the Monet style image generator is a Monet image, the output should get the same image.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.279054Z","iopub.execute_input":"2022-08-29T20:49:52.279310Z","iopub.status.idle":"2022-08-29T20:49:52.289179Z","shell.execute_reply.started":"2022-08-29T20:49:52.279283Z","shell.execute_reply":"2022-08-29T20:49:52.288138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training \nLet's initialize the optimizers for the models in strategy.scope() because we are using TPU","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.290312Z","iopub.execute_input":"2022-08-29T20:49:52.290559Z","iopub.status.idle":"2022-08-29T20:49:52.299047Z","shell.execute_reply.started":"2022-08-29T20:49:52.290532Z","shell.execute_reply":"2022-08-29T20:49:52.298484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compiling and fitting the model**","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.300268Z","iopub.execute_input":"2022-08-29T20:49:52.300465Z","iopub.status.idle":"2022-08-29T20:49:52.361356Z","shell.execute_reply.started":"2022-08-29T20:49:52.300442Z","shell.execute_reply":"2022-08-29T20:49:52.360709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = cycle_gan_model.fit(final_dataset, \n                        epochs=EPOCHS, \n                        ).history","metadata":{"execution":{"iopub.status.busy":"2022-08-29T20:49:52.362558Z","iopub.execute_input":"2022-08-29T20:49:52.362958Z","iopub.status.idle":"2022-08-29T21:04:22.855070Z","shell.execute_reply.started":"2022-08-29T20:49:52.362927Z","shell.execute_reply":"2022-08-29T21:04:22.854051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing loss function ","metadata":{}},{"cell_type":"code","source":"loss_results_df = pd.DataFrame(history)\nloss_results_df = loss_results_df.applymap(np.mean)\nplt.plot(loss_results_df.index, loss_results_df['monet_gen_loss'], color='g', label='Loss Monet Generator')\nplt.plot(loss_results_df.index, loss_results_df['photo_gen_loss'], color='r', label='Loss Photo Generator')\nplt.plot(loss_results_df.index, loss_results_df['monet_disc_loss'], color='b', label='Loss Monet Discriminator')\nplt.plot(loss_results_df.index, loss_results_df['photo_disc_loss'], color='m', label='Loss Photo Discriminator')\nplt.legend(loc='best')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T21:04:22.858798Z","iopub.execute_input":"2022-08-29T21:04:22.859071Z","iopub.status.idle":"2022-08-29T21:04:23.088245Z","shell.execute_reply.started":"2022-08-29T21:04:22.859039Z","shell.execute_reply":"2022-08-29T21:04:23.087393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(5, 2, figsize=(10, 10))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T21:04:23.089886Z","iopub.execute_input":"2022-08-29T21:04:23.090221Z","iopub.status.idle":"2022-08-29T21:04:26.044204Z","shell.execute_reply.started":"2022-08-29T21:04:23.090183Z","shell.execute_reply":"2022-08-29T21:04:26.043375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As you can see the results are not that great. Please increase the number of epochs to about 100 for a better result. The dicriminator loss tends to be around 0.55 which is much better than what we have achieved here.**","metadata":{}},{"cell_type":"markdown","source":"## Saving and Submission","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images\n\ni = 1\nfor img in photo_ds:\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1\n    \n\nimport shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2022-08-29T21:04:26.045696Z","iopub.execute_input":"2022-08-29T21:04:26.046178Z","iopub.status.idle":"2022-08-29T21:38:37.480472Z","shell.execute_reply.started":"2022-08-29T21:04:26.046132Z","shell.execute_reply":"2022-08-29T21:38:37.479537Z"},"trusted":true},"execution_count":null,"outputs":[]}]}